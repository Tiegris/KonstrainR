\chapter{Case study 1: Enforcing general best practices}
\label{chap:case_study1}

This chapter showcases the capabilities of the Konstrainer framework through a fictional company's business application. The case study begins by establishing the starting state of a demo application. It then identifies several problems with the application, proposes solutions to those problems, and finally establishes rules that prevent the future occurrence of those problems.

A fictional company, Apples Inc., embarked on a journey to modernize its business operations. The mangers decided that buying an existing solution is too expensive, so they opted for a custom application built by their engineers. They specifically mandated that the application be built upon the microservices architecture and run in Kubernetes, mainly because this is trendy today, even thought they did not properly investigate whether it is the best solution for their use-case or not. Moreover, the engineers lacked prior experience with Kubernetes. The combination of insufficient experience, inadequate planning, and tight deadlines led to numerous mistakes in the development process.

Their application is still a work in progress, with limited functionality - it serves as a small demo solely for the purpose of this case study.

\section{Setup}

I have prepared this demo on GitHub: \url{https://github.com/Tiegris/KonstrainR/tree/main/main/DemoApp}

The repository is publicly available, enabling readers to download it, try it out, and follow along with this demonstration. 

Requirements:

\begin{itemize}
    \item Text editor
    \item Docker
    \item Sandbox Kubernetes cluster (Tested with version: v1.27.2)
    \item Helm
    \item yq (Tested with version: (\url{github.com/mikefarah/yq/}) version 4.18.)
\end{itemize}

The demo was tested on Ubuntu 22 with bash.

If you follow along, make sure that you are in the \emph{DemoApp} folder.

Optionally, you can build the Docker images for yourself, but they are publicly available, so this is optional. Run this, so the setups script builds the images for you. If you do this, you have to manually edit the items.yaml and users.yaml files to use your images.

\begin{lstlisting}[caption={Optional: Build images},language=bash,label=code:bash1]
export BUILD_IMAGES=1
export REPO=<your-docker.io-account-name>
\end{lstlisting}

Execute the initial setup script: \texttt{./initial-setup.sh} This script establishes the initial state of the demo system by replicating commands that the developers of the system might have used, or running commands that simulate steps of the development process.

\subsection{Breakdown of setup}

This section breaks down the steps of the setup script and explains each step, by telling a story what the developers did, to achieve the current state of the system.

When the initial two microservices for the app were developed, the developers created a YAML file for each and defined the essential Kubernetes resources required for them:

\begin{lstlisting}[caption={Create first deployment},language=bash,label=code:bash3]
# Initial creation of users app
kubectl create ns users
kubectl apply -n users -f $home/k8s/users.yaml

# Initial creation of items app
kubectl create ns items
kubectl apply -n items -f $home/k8s/items.yaml
\end{lstlisting}

TODO yaml files???

The resource descriptions were generated using the VS Code Kubernetes extension and slightly customized, so most values were left on default.

Note that the items.yaml contains an unused PVC. This could be the result of a copy and paste oversight, or just someone forgot to delete it. We will use Konstrainer to detect it.

The `items' app went through a long development. The setup script simulates the multiple redeployment of the app like this:

\begin{lstlisting}[caption={Create first deployment},language=bash,label=code:bash4]
# Simulate a history of changes
for v in {1..14}; do
    yq eval 'select(.kind == "Deployment").spec.template.metadata.annotations.v = env(v)' $home/k8s/items.yaml -i
    kubectl apply -n items -f $home/k8s/items.yaml
done
# Restore yaml file
yq eval 'select(.kind == "Deployment").spec.template.metadata.annotations.v = "0"' $home/k8s/items.yaml -i
\end{lstlisting}

The engineers in charge of the Kubernetes deployments set up test environments for experimentation and testing purposes, but they neglected to delete them.

This is simulated by these commands:

\begin{lstlisting}[caption={Create first deployment},language=bash,label=code:bash5]
# Simulate John Athan creating test namespace
kubectl create ns jathan-test || true
# Simulate Ida Red creating test namespace and leaving some leftover junk in it
kubectl create ns ired-test || true
kubectl apply -n ired-test -f $home/k8s/junk.yamlb     
\end{lstlisting}

The developers decided that they want to use a NoSQL database. They searched the internet and one of the first search results was this: \url{bitnami.com/stack/mongodb/helm}. This website shows a command which looks like it deploys a MongoDB. The engineers tested it, and it worked. They did not bother with reading the documentation or properly configuring the database, there was no time for that.

\begin{lstlisting}[caption={Create first deployment},language=bash,label=code:bash5]
kubectl create ns mongo
helm install -n mongo my-release \
  oci://registry-1.docker.io/bitnamicharts/mongodb
\end{lstlisting}

The output of the command has some helpful tips. The engineers deploying MongoDB followed them like this:

\begin{lstlisting}[caption={Create first deployment},language=bash,label=code:bash6]
export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace mongo \
  my-release-mongodb -o jsonpath="{.data.mongodb-root-password}" \
  | base64 -d)
export MONGO_HOST="my-release-mongodb.mongo.svc.cluster.local"
\end{lstlisting}

They configured the `items' service to use MongoDB. They manually edited the items.yaml file and changed the code based on the first tutorial they could find. The image used by the `items' deployment already uses MongoDB, the commands in the \ref{code:bash7} code snippet only emulate this process.

\begin{lstlisting}[caption={Configure the `items' app to use MongoDB},language=bash,label=code:bash7]
yq eval 'select(.kind == "Deployment").spec.template.spec.containers[0].env[0].value = env(MONGODB_ROOT_PASSWORD)' $home/k8s/items.yaml -i
yq eval 'select(.kind == "Deployment").spec.template.spec.containers[0].env[1].value = env(MONGO_HOST)' $home/k8s/items.yaml -i
kubectl apply -n items -f $home/k8s/items.yaml
\end{lstlisting}

At this point, we have a demo application running which resembles a system with many mistakes.

\subsection{Testing the app}

TODO

Check if you done everything correctly.

Let's test the app.

\begin{lstlisting}[caption={TODO},language=bash,label=code:bash8]
kubectl port-forward -n items service/items 8080:8080
# In a separate terminal
curl http://localhost:8080/items
\end{lstlisting}

If you got some valid JSON response, that means the setup was successful.

TODO pods status

\begin{lstlisting}[caption={Teardown},language=bash,label=code:bash9]
helm uninstall my-release -n mongo
kubectl delete ns users items jathan-test ired-test mongo
\end{lstlisting}

\section{Running Konstrainer}

Install the Konstrainer helm chart.

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]
cd ../charts
kubectl create ns konstrainer-ns
helm upgrade konstrainr-core konstrainr-core -n konstrainer-ns --install
\end{lstlisting}

Wait for it to start, then port forward the core component.

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]
kubectl port-forward service/konstrainer-core 8443:8080 -n konstrainer-ns
\end{lstlisting}

\subsection{Basic Diagnostics}

First let's run the basic diagnostics on the cluster.

The Basic Diagnostics is a build dsl file, which generates a report of the most common best-practice violations in the cluster.

Go to \url{https://localhost:8443}. The site has a self signed certificate, configure your browser to trust it. For login use \emph{admin:admin}. Upload the basic diagnostics dsl:
\url{../Konstrainer-Dsl/src/main/kotlin/me/btieger/builtins/BasicDiagnostic.kt}
Use browse, than submit. Wait for it to compile. The site does not refresh automatically, refresh the page after about 20 seconds to see if it's compiled. You should see: `Build status: Ready'

Click `Start Server'. Wait for it to start. After about 5 seconds refresh the page. You should see: `Server status: Up'

Click `Monitors' in the navigation menu. You should see the report generated by the BasicDiagnostic. Let's inspect it.

Here we can see many of the problem with our cluster.

Read the report like this:

- The outermost box is an agent. It is a server instance running in your cluster that generates reports or enforces constraints.

- It contains many aggregation groups. An aggregation group is a bundle of problems usually about the same kind of Kubernetes resources. The first aggregation group in this report is the: `Services`

  - An aggregation group holds Kubernetes resources which have problems. Resources with problems are called marked resources. In this case the only marked resource in the `Services` aggregation group is the: `services/users.users'

  - The format of the marked Kubernetes resources are: `apiKind/name.namespace'

  - The body of the box lists the problems separated by commas. In this case the only problem is: `No backend'

The `No backend` mark means that the service has no pods at all.

The users service was deployed in the using the `initial-setup.sh` and it is defined in the [./k8s/users.yaml](./k8s/users.yaml) file.

If we inspect this file very carefully, we can see that the selector of the service is looking for pods with the labels: `app: users`, but the deployment defines pods with the label: `app: user`. Honestly this is a very easy typo to make, so in this case the Konstrainer discovered a bug.

The next aggregation group is the `Deployments`. The most common tags are:

- No resources defined: This means that not all containers in the pods of the deployment have resource requests and limits set. This is a warning, it is usually  good practice to set the values.
- No node selector: This means that there is no logic provided on which node (usually a virtual machine) should the pods of the deployment run.
- No probes: No liveness probes are defined. Without probes the Kubernetes api can not determine if a pod is healthy or not, so it is a good practice to define probes.
- Has long history: The revisionHistoryLimit of the deployment is set to more than 4. The revisionHistoryLimit determines how many replicasets should the deployment keep as a history. The default is 10, but if it is too many, it can result in a lot of unused resources which can put an unnecessary load on the Kubernetes api, so it is a good practice to set it lower than the default.

The next aggregation group with problems is the PVCs. It tells us that there is a PersistentVolumeClaim, which is unused. This pvc is defined in the ./k8s/items.yaml file. If we inspect this file we can see that this pvc is in fact not used. It is probably the result of a copy paste error or some kind of leftover from the evolution of the project. Dangling pvcs can be very expensive. Storage is cheap, but if many pvcs claim some storage from a cloud provider and it is unused, the price can add up quickly.

The next group is the namespaces. It says that the jathan-test namespace has no pods. It is common for developers to create a test namespace which they can use s a sandbox, but often when they are finished with their work, they leave it behind and forgot to delete it. In our case the engineer John Athan probably forgot o delete his sandbox namespace.

The last group is the Pods. It has two items. The first is the pod 'my-test' in the ired-test namespace. It is a dangling pod, which means that there is no operator (e.g.: a deployment or statefulset) managing its lifecycle. The seconds pod has way more problems. It is not running because the docker image of the pod could not be pulled. In this case both pods were created by a developer for testing or development purposes, but she forgot to delete them.

\subsection{Introducing basic rule enforcement}

Upload and start the \url{../Konstrainer-Dsl/src/main/kotlin/me/btieger/builtins/BasicEnforce.kt} dsl file.

This file describes basic rules that are evaluated upon events, like deployment creation. This can be used to prevent the creation of poorly configured resources, and can enforce best practices from the beginning.

Let's deploy the new component of the company app. First, let's create a namespace for it:

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]
cd DemoApp
kubectl apply -f k8s/ns.yaml
\end{lstlisting}

Now let's deploy the new app, but intentionally forget to switch to the newly created namespace:

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]
# make sure you are in the default namespace
kubectl config set-context --current --namespace=default
kubectl apply -f k8s/accounting.yaml
\end{lstlisting}

You should get a warning: `Warning: You are working in the namespace: default`

If you work in the default namespace that might be a mistake, so the BasicDiagnostics warns you. Let's revert our mistake and switch to the correct namespace:

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]
kubectl delete deployment.apps/accounting
kubectl apply -f k8s/accounting.yaml --namespace=accounting
\end{lstlisting}

You should get the following error:

\begin{lstlisting}[caption={TODO},language=bash,label=code:todo]
Error from server: error when creating "k8s/accounting.yaml": admission webhook "node-affinity.btieger.me" denied the request: Deployment must have some kind of node affinity! (affinity, nodeSelector, nodeName)
\end{lstlisting}

It is generally a good practice to define some kind of logic on where to schedule our pods. There can be many principals, this rule only requires to define something. In a real scenario we should define at least a nodeSelector, but for now lets just set the nodeName field.

Depending on your cluster find a suitable node:

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]
kubectl get nodes -A
# chose one, eg.:
export node_name="docker-desktop"
yq eval 'select(.kind == "Deployment").spec.template.spec.nodeName = env(node_name)' k8s/accounting.yaml -i
kubectl apply -f k8s/accounting.yaml --namespace=accounting
\end{lstlisting}

Now we fixed the node affinity problem, but there are still problems:

\begin{lstlisting}[caption={TODO},language=bash,label=code:todo]
Error from server: error when creating "k8s/accounting.yaml": admission webhook "deny-no-resources.btieger.me" denied the request: Deployment must have resource definitions!
\end{lstlisting}

It is possible to define resource requests and limitations for Pods. Here is a great blog post why defining them a best practice: [Kubernetes best practices: Resource requests and limits](cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits)

Lets set some resource limits and requests, than redeploy the deployment:

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]
yq eval 'select(.kind == "Deployment").spec.template.spec.containers[0].resources.limits.cpu = "500m"' k8s/accounting.yaml -i
yq eval 'select(.kind == "Deployment").spec.template.spec.containers[0].resources.limits.memory = "128Mi"' k8s/accounting.yaml -i
yq eval 'select(.kind == "Deployment").spec.template.spec.containers[0].resources.requests.cpu = "500m"' k8s/accounting.yaml -i
yq eval 'select(.kind == "Deployment").spec.template.spec.containers[0].resources.requests.memory = "128Mi"' k8s/accounting.yaml -i
kubectl apply -f k8s/accounting.yaml --namespace=accounting
\end{lstlisting}

This time it succeeded, but we got some warnings:

\begin{lstlisting}[caption={TODO},language=bash,label=code:todo]
Warning: No security context
Warning: RevisionHistoryLimit was set to 4, from original: 10
\end{lstlisting}

The first just tells us tht we have not yet defined a security context, the seconds tells us that the revisionHistoryLimit of the deployment was changed. Konstrainer allows to create rules that not only reject some actions, but do some minor alterations as well.

Describe the newly created deployment and we can see that the revisionHistoryLimit in fact got changed:

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]
kubectl get deployment accounting -n accounting -o yaml | yq '.spec.revisionHistoryLimit'
\end{lstlisting}


TODO

This is the end of the demo. In this demo we saw the basic capabilities of the Konstrainer platform, some use-cases, and how to create our own rules.
