\chapter{Case study 1: Enforcing general best practices}
\label{chap:case_study1}

This chapter showcases the capabilities of the Konstrainer framework through a fictional company's business application. The case study begins by establishing the starting state of a demo application. It then identifies several problems with the application, proposes solutions to those problems, and finally establishes rules that prevent the future occurrence of those problems.

A fictional company, Apples Inc., embarked on a journey to modernize its business operations. The mangers decided that buying an existing solution is too expensive, so they opted for a custom application built by their engineers. They specifically mandated that the application be built upon the microservices architecture and run in Kubernetes, mainly because this is trendy today, even thought they did not properly investigate whether it is the best solution for their use-case or not. Moreover, the engineers lacked prior experience with Kubernetes. The combination of insufficient experience, inadequate planning, and tight deadlines led to numerous mistakes in the development process.

Their application is still a work in progress, with limited functionality - it serves as a small demo solely for the purpose of this case study.

\section{Setup}

I have prepared this demo on GitHub: \url{https://github.com/Tiegris/KonstrainR/tree/main/main/DemoApp}

The repository is publicly available, enabling readers to download it, try it out, and follow along with this demonstration. If you do not wish to check out the repository, the referenced resource files can also be found in the appendix.

Requirements:

\begin{itemize}
    \item Docker
    \item Playground Kubernetes cluster (Tested with version: v1.27.2)
    \item Helm
    \item yq (Tested with version: (\url{github.com/mikefarah/yq/}) version 4.18.)
\end{itemize}

The demo was tested on Ubuntu 22 with bash.

If you follow along, make sure that you are in the \emph{main/DemoApp} folder of the repository.

Execute the initial setup script: \lstinline|./initial-setup.sh| This script establishes the initial state of the demo system by replicating commands that the developers of the system might have used, or running commands that simulate steps of the development process.

\subsection{Breakdown of setup}

This section breaks down the steps of the setup script and explains each step, by telling a story what the developers did, to achieve the current state of the system.

When the initial two microservices for the app were developed, the developers created a YAML file for each and defined the essential Kubernetes resources required for them:

\begin{lstlisting}[caption={Create first deployment},language=bash,label=code:bash3]
# Initial creation of users app
kubectl create ns users
kubectl apply -n users -f $home/k8s/users.yaml

# Initial creation of items app
kubectl create ns items
kubectl apply -n items -f $home/k8s/items.yaml
\end{lstlisting}

The resource descriptions were generated using the VS Code Kubernetes extension\footnote{\url{https://github.com/vscode-kubernetes-tools/vscode-kubernetes-tools}} and slightly customized, so most values were left on default.

Note that the items.yaml (\ref{appendix:csr:items}) contains an unused \emph{PersistentVolumeClaim} (PVC). This could be the result of a copy and paste oversight, or just someone forgot to delete it. We will use Konstrainer to detect it.

The `items' app went through a long development. The setup script simulates the multiple redeployment of the app, as shown in the \ref{code:bash4} snippet.

\begin{lstlisting}[caption={Simulate redeployment of the `items' app},language=bash,label=code:bash4]
# Simulate a history of changes
for v in {1..14}; do
    yq eval 'select(.kind == "Deployment").spec.template.metadata.annotations.v = env(v)' $home/k8s/items.yaml -i
    kubectl apply -n items -f $home/k8s/items.yaml
done
# Restore yaml file
yq eval 'select(.kind == "Deployment").spec.template.metadata.annotations.v = "0"' $home/k8s/items.yaml -i
\end{lstlisting}

The engineers in charge of the Kubernetes deployments set up test environments for experimentation and testing purposes, but they neglected to delete them. This is simulated by the \ref{code:bash5} code snippet.

\begin{lstlisting}[caption={Simulate leftover resources},language=bash,label=code:bash5]
  # Simulate John Athan creating test namespace
  kubectl create ns jathan-test || true
  # Simulate Ida Red creating test namespace and leaving some leftover resources in it
  kubectl create ns ired-test || true
  kubectl apply -n ired-test -f $home/k8s/junk.yaml
\end{lstlisting}

The developers decided that they want to use a NoSQL database. They searched the internet, and one of the first search results was this: \url{bitnami.com/stack/mongodb/helm}. This website shows a command that looks like it deploys a MongoDB. The engineers tested it, and it worked. They did not bother with reading the documentation or properly configuring the database; there was no time for that. The \ref{code:bash5b} snippet shows how they used the code from the website.

\begin{lstlisting}[caption={Deploy MongoDB},language=bash,label=code:bash5b]
kubectl create ns mongo
helm install -n mongo my-release \
  oci://registry-1.docker.io/bitnamicharts/mongodb
\end{lstlisting}

After deploying MongoDB, the engineers configured the `items' service to use it. They manually edited the items.yaml file and changed the code based on the first tutorial they could find. The image used by the `items' deployment already uses MongoDB; the commands in the \ref{code:bash7} code snippet only emulate this process.

\begin{lstlisting}[caption={Configuring the `items' app to use MongoDB},language=bash,label=code:bash7]
export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace mongo \
  my-release-mongodb -o jsonpath="{.data.mongodb-root-password}" \
  | base64 -d)
export MONGO_HOST="my-release-mongodb.mongo.svc.cluster.local"
yq eval 'select(.kind == "Deployment").spec.template.spec.containers[0].env[0].value = env(MONGODB_ROOT_PASSWORD)' $home/k8s/items.yaml -i
yq eval 'select(.kind == "Deployment").spec.template.spec.containers[0].env[1].value = env(MONGO_HOST)' $home/k8s/items.yaml -i
kubectl apply -n items -f $home/k8s/items.yaml
\end{lstlisting}

At this point, we have a demo application running which resembles a system with many mistakes.

\subsection{Test and teardown the demo application}

If you have followed along, make sure to test the deployment. First check the status of all \emph{Pod}s, by executing: \lstinline|kubectl get pods -A| All the \emph{Pod}s should have the status: `Running', except the `my-test-2' \emph{Pod}.

You can also test the integration of the items app and the MongoDB by executing the commands in the \ref{code:bash8} snippet.

\begin{lstlisting}[caption={Test the integration of the items app and MongoDB},language=bash,label=code:bash8]
kubectl port-forward -n items service/items 8080:8080
# In a separate terminal
curl http://localhost:8080/items
# You should get a valid JSON as result
\end{lstlisting}

If you wish to uninstall the demo and the Konstrainer application run the code in the \ref{code:bash9} snippet.

\begin{lstlisting}[caption={Teardown},language=bash,label=code:bash9]
# Uninstall demo app
helm uninstall my-release -n mongo
kubectl delete ns users items jathan-test ired-test mongo
# Uninstall Konstrainer
helm uninstall konstrainr-core -n konstrainer-ns
kubectl delete ns konstrainer-ns
\end{lstlisting}

\section{Introduce policies}

To follow along, you can install the Konstrainer application using a helm chart, as shown in the \ref{code:bashk1} code snippet.

\begin{lstlisting}[caption={Install Konstrainer},language=bash,label=code:bashk1]
cd ../charts
kubectl create ns konstrainer-ns
helm upgrade konstrainr-core konstrainr-core -n konstrainer-ns --install
\end{lstlisting}

Wait for it to start, then port forward the core component: \lstinline|kubectl port-forward service/konstrainer-core 8443:8080 -n konstrainer-ns|
This will allow you to access its frontend in your browser.

\subsection{Basic Diagnostics}

I have collected a set of best practices, and implemented them in my DSL. I implemented them in two files, the first file contains a script which creates a diagnostics report on the cluster including potential bugs and bad practices. This is the \emph{basic diagnostic} script.

If you visit the Konstrainer Web UI at \url{https://localhost:8443}, and configure your browser to trust the self-signed certificate you should see the TODO ref image. For login use \emph{admin:admin}. 

TODO insert webui screenshot here


Upload the basic diagnostics script, which can be found in the repository at:
\url{$PROJECT_ROOT/main/Konstrainer-Dsl/src/main/kotlin/me/btieger/builtins/BasicDiagnostic.kt}
Use browse, than submit. Wait for it to compile. The site does not refresh automatically, refresh the page after about 20 seconds to see if it's compiled. You should see: `Build status: Ready'

Click `Start Server'. Wait for it to start. After about 5 seconds refresh the page. You should see: `Server status: Up'

Click `Monitors' in the navigation menu. You should see the report generated by the BasicDiagnostic. Let's inspect it.

Here we can see many of the problem with our cluster.

TODO report screenshot

Read the report like this:

- The outermost box is an agent. It is a server instance running in your cluster that generates reports or enforces constraints.

- It contains many aggregation groups. An aggregation group is a bundle of problems usually about the same kind of Kubernetes resources. The first aggregation group in this report is the: `Services`

  - An aggregation group holds Kubernetes resources which have problems. Resources with problems are called marked resources. In this case the only marked resource in the `Services` aggregation group is the: `services/users.users'

  - The format of the marked Kubernetes resources are: `apiKind/name.namespace'

  - The body of the box lists the problems separated by commas. In this case the only problem is: `No backend'

The `No backend` mark means that the service has no pods at all.

The users service was deployed in the using the `initial-setup.sh` and it is defined in the [./k8s/users.yaml](./k8s/users.yaml) file.

If we inspect this file very carefully, we can see that the selector of the service is looking for pods with the labels: `app: users`, but the deployment defines pods with the label: `app: user`. Honestly this is a very easy typo to make, so in this case the Konstrainer discovered a bug.

The next aggregation group is the `Deployments`. The most common tags are:

- No resources defined: This means that not all containers in the pods of the deployment have resource requests and limits set. This is a warning, it is usually  good practice to set the values.
- No node selector: This means that there is no logic provided on which node (usually a virtual machine) should the pods of the deployment run.
- No probes: No liveness probes are defined. Without probes the Kubernetes api can not determine if a pod is healthy or not, so it is a good practice to define probes.
- Has long history: The revisionHistoryLimit of the deployment is set to more than 4. The revisionHistoryLimit determines how many replicasets should the deployment keep as a history. The default is 10, but if it is too many, it can result in a lot of unused resources which can put an unnecessary load on the Kubernetes api, so it is a good practice to set it lower than the default.

The next aggregation group with problems is the PVCs. It tells us that there is a PersistentVolumeClaim, which is unused. This pvc is defined in the ./k8s/items.yaml file. If we inspect this file we can see that this pvc is in fact not used. It is probably the result of a copy paste error or some kind of leftover from the evolution of the project. Dangling pvcs can be very expensive. Storage is cheap, but if many pvcs claim some storage from a cloud provider and it is unused, the price can add up quickly.

The next group is the namespaces. It says that the jathan-test namespace has no pods. It is common for developers to create a test namespace which they can use s a sandbox, but often when they are finished with their work, they leave it behind and forgot to delete it. In our case the engineer John Athan probably forgot o delete his sandbox namespace.

The last group is the Pods. It has two items. The first is the pod 'my-test' in the ired-test namespace. It is a dangling pod, which means that there is no operator (e.g.: a deployment or statefulset) managing its lifecycle. The seconds pod has way more problems. It is not running because the docker image of the pod could not be pulled. In this case both pods were created by a developer for testing or development purposes, but she forgot to delete them.

\subsection{Introducing basic rule enforcement}

The second script I prepared for the case study does not create a report, but rather enforces best practices by intercepting requests to the Kubernetes API.
This file describes basic rules that are evaluated upon events, like deployment creation. This can be used to prevent the creation of poorly configured resources, and can enforce best practices from the beginning.

To follow along, upload the \emph{basic enforce} script located at the same directory as the \emph{basic diagnostics} script.

Let's deploy the new component of the company app. First, let's create a namespace for it:

TODO managed label

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]
cd DemoApp
kubectl apply -f - <<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: accounting
  labels:
    managed: "true"
EOF
\end{lstlisting}

Now let's deploy the new app, but intentionally forget to switch to the newly created namespace:

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]

# Make sure you are in the default namespace
kubectl config set-context --current --namespace=default
kubectl apply -f k8s/accounting.yaml
\end{lstlisting}

You should get a warning: `Warning: You are working in the namespace: default`

If you work in the default namespace that might be a mistake, so the BasicDiagnostics warns you. Let's revert our mistake and switch to the correct namespace:

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]
kubectl delete deployment.apps/accounting
kubectl apply -f k8s/accounting.yaml --namespace=accounting
\end{lstlisting}

You should get the following error:

\begin{lstlisting}[caption={TODO},language=bash,label=code:todo]
Error from server: error when creating "k8s/accounting.yaml": admission webhook "node-affinity.btieger.me" denied the request: Deployment must have some kind of node affinity! (affinity, nodeSelector, nodeName)
\end{lstlisting}

It is generally a good practice to define some kind of logic on how to place our pods. There can be many principals, this rule only requires to define something. In a real scenario we should define at least a nodeSelector, but for now lets just set the nodeName field.

Depending on your cluster find a suitable node:

\begin{lstlisting}[caption={Fix node affinity error},language=bash,label=code:bashx]
kubectl get nodes -A
# chose one, eg.:
export node_name="docker-desktop"
yq eval 'select(.kind == "Deployment").spec.template.spec.nodeName = env(node_name)' k8s/accounting.yaml -i
kubectl apply -f k8s/accounting.yaml --namespace=accounting
\end{lstlisting}

Now we fixed the node affinity problem, but there are still other problems:

\begin{lstlisting}[caption={TODO},language=bash,label=code:todo]
Error from server: error when creating "k8s/accounting.yaml": admission webhook "deny-no-resources.btieger.me" denied the request: Deployment must have resource definitions!
\end{lstlisting}

It is possible to define resource requests and limitations for Pods. Here is a great blog post why defining them a best practice: [Kubernetes best practices: Resource requests and limits](cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits)

Let's set some resource limits and requests, then redeploy the deployment:

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]
yq eval 'select(.kind == "Deployment").spec.template.spec.containers[0].resources.limits.cpu = "500m"' k8s/accounting.yaml -i
yq eval 'select(.kind == "Deployment").spec.template.spec.containers[0].resources.limits.memory = "128Mi"' k8s/accounting.yaml -i
yq eval 'select(.kind == "Deployment").spec.template.spec.containers[0].resources.requests.cpu = "500m"' k8s/accounting.yaml -i
yq eval 'select(.kind == "Deployment").spec.template.spec.containers[0].resources.requests.memory = "128Mi"' k8s/accounting.yaml -i
kubectl apply -f k8s/accounting.yaml --namespace=accounting
\end{lstlisting}

This time it succeeded, but we got some warnings:

\begin{lstlisting}[caption={TODO},language=bash,label=code:todo]
Warning: No security context
Warning: RevisionHistoryLimit was set to 4, from original: 10
\end{lstlisting}

The first just tells us tht we have not yet defined a security context, the seconds tells us that the revisionHistoryLimit of the deployment was changed. Konstrainer allows to create rules that not only reject some actions, but do some minor alterations as well.

Describe the newly created deployment and we can see that the revisionHistoryLimit in fact got changed:

\begin{lstlisting}[caption={TODO},language=bash,label=code:bashx]
kubectl get deployment accounting -n accounting -o yaml | yq '.spec.revisionHistoryLimit'
\end{lstlisting}

This chapter showed the motivation behind the Konstrainer project by showcasing some common easy mistakes and how easy it is to make them. After that it demonstrated the power of the Konstrainer platform and showed the capabilities of my pre-made scripts. 




TODO
